{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 64-bit ('torch-1.7': venv)"
  },
  "interpreter": {
   "hash": "4d9226719b86cea56c5e36d5c6edd64f676af5a2976ce4e3da1f4328ac49da39"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/engine210/MMFinal/torch-1.7/lib/python3.8/site-packages/torchvision/transforms/transforms.py:280: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('clip-ViT-B-32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_data(file_name):\n",
    "    with open(file_name) as f:\n",
    "        article_list = [json.loads(line) for line in f]\n",
    "        return article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/home/engine210/MMFinal/'\n",
    "DATA_DIR = '/home/engine210/MMFinal/data/'\n",
    "TARGET_DIR = \"/home/engine210/MMFinal/viz/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = read_json_data(os.path.join(DATA_DIR, 'mmsys_anns', 'public_test_mmsys_final.json'))\n",
    "v_data = test_samples[445]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(v_data):\n",
    "    img_path = os.path.join(DATA_DIR, v_data[\"img_local_path\"])\n",
    "    img = Image.open(img_path)\n",
    "    img_shape = img.size\n",
    "\n",
    "    bbox_list = v_data['maskrcnn_bboxes']\n",
    "    bbox_list.append([0, 0, img_shape[0], img_shape[1]])  # For entire image (global context)\n",
    "\n",
    "    cap1 = v_data['caption1_modified']\n",
    "    cap2 = v_data['caption2_modified']\n",
    "    # cap1 = v_data['caption1']\n",
    "    # cap2 = v_data['caption2']\n",
    "\n",
    "    print(cap1)\n",
    "    print(cap2)\n",
    "    print(img.size)\n",
    "    score_c1 = []\n",
    "    score_c2 = []\n",
    "\n",
    "    for i, box in enumerate(bbox_list):\n",
    "        box = list(map(int, box))\n",
    "        if box[0] == box[2]: box[2] += 1\n",
    "        if box[1] == box[3]: box[3] += 1\n",
    "        crop_img = img.crop(box)\n",
    "        b = BytesIO()\n",
    "        crop_img.save(b,format=\"jpeg\")\n",
    "        crop_img = Image.open(b)\n",
    "        # crop_img.save('tmp'+str(i)+'.jpg')\n",
    "        crop_img_emb = model.encode(crop_img)\n",
    "        text_emb = model.encode([cap1, cap2])\n",
    "\n",
    "        # #Compute cosine similarities \n",
    "        cos_scores = util.cos_sim(crop_img_emb, text_emb)\n",
    "        # print(i, cos_scores.numpy()[0])\n",
    "        score_c1.append(cos_scores.numpy()[0][0])\n",
    "        score_c2.append(cos_scores.numpy()[0][1])\n",
    "    return score_c1, score_c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "President PERSON's ORDINAL wife, PERSON, died in a plane crash in DATE.\n",
      "There is no truth to the sensational headline that PERSONâ€™s Ex-Wife PERSON Died After Airline Accident\n",
      "(865, 452)\n",
      "7 0\n"
     ]
    }
   ],
   "source": [
    "score_c1, score_c2 = get_scores(v_data)\n",
    "print(score_c1.index(max(score_c1)), score_c2.index(max(score_c2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 139
    }
   ],
   "source": [
    "v_data['context_label'] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bboxes(data):\n",
    "    box_color = (0, 255, 0)  # Green\n",
    "    line_thickness = 5\n",
    "    img_path = os.path.join(DATA_DIR, v_data['img_local_path'])\n",
    "    img = cv2.imread(img_path)\n",
    "    bboxes = data[\"maskrcnn_bboxes\"]\n",
    "\n",
    "    for i, bbox in enumerate(bboxes):\n",
    "        img = cv2.rectangle(img, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), box_color, line_thickness)\n",
    "    cv2.imwrite('tmp.jpg', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_bboxes(v_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(os.path.join(DATA_DIR, v_data['img_local_path']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = img.crop(v_data['maskrcnn_bboxes'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1.save('tmp1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[280.65081787109375,\n",
       "  24.754322052001953,\n",
       "  525.8712768554688,\n",
       "  396.9060974121094],\n",
       " [61.683773040771484,\n",
       "  68.53385162353516,\n",
       "  251.91490173339844,\n",
       "  397.5182189941406],\n",
       " [386.9053955078125, 311.1746520996094, 422.8839416503906, 399.09246826171875],\n",
       " [562.51953125, 123.96571350097656, 600.0, 275.7099304199219],\n",
       " [230.67855834960938, 201.62765502929688, 287.44677734375, 389.9917907714844],\n",
       " [385.1832275390625, 311.67584228515625, 422.2655334472656, 383.9637756347656],\n",
       " [225.81739807128906,\n",
       "  184.658935546875,\n",
       "  252.52146911621094,\n",
       "  237.91329956054688],\n",
       " [384.7476806640625, 165.92344665527344, 505.6837158203125, 396.7842712402344],\n",
       " [216.56350708007812,\n",
       "  170.42654418945312,\n",
       "  262.8428039550781,\n",
       "  237.44952392578125],\n",
       " [259.2336120605469, 230.63587951660156, 286.9748229980469, 258.78125]]"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "v_data['maskrcnn_bboxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "error",
     "evalue": "OpenCV(4.5.2) /tmp/pip-req-build-13uokl4r/opencv/modules/highgui/src/window.cpp:404: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'imshow'\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-1bef0765dc20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdraw_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-474638902aeb>\u001b[0m in \u001b[0;36mdraw_bboxes\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# for i, bbox in enumerate(bboxes):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#     img = cv2.rectangle(img, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), box_color, line_thickness)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'img'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.2) /tmp/pip-req-build-13uokl4r/opencv/modules/highgui/src/window.cpp:404: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'imshow'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'score_c1' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3d8a338f2e43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maskrcnn_bboxes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtop_bbox_c1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_bbox_from_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_c1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtop_bbox_c2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_bbox_from_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_c2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_bbox_c1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_bbox_c2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'score_c1' is not defined"
     ]
    }
   ],
   "source": [
    "bboxes = v_data['maskrcnn_bboxes']\n",
    "top_bbox_c1 = top_bbox_from_scores(bboxes, score_c1)\n",
    "top_bbox_c2 = top_bbox_from_scores(bboxes, score_c2)\n",
    "print(top_bbox_c1, top_bbox_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_c1, score_c2 = get_scores(v_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([-1.3500,  0.1246, -3.7564,  5.0216, -1.3501, -1.4075, -1.2110, -1.2906,\n        -1.3245, -2.4942,  2.6325], device='cuda:0') tensor([-3.2680,  1.2437, -3.6021,  4.2952, -1.1264,  3.6721, -1.2736, -0.7548,\n        -3.2077, -1.1968,  6.7783], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(score_c1, score_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[389, 72, 505, 373], [89, 312, 190, 396], [116, 225, 161, 288], [180, 225, 207, 271], [579, 193, 597, 249], [217, 225, 256, 267], [67, 237, 92, 275], [29, 240, 64, 276], [229, 298, 251, 330], [89, 205, 104, 228], [0, 0, 600, 400]]\n"
     ]
    }
   ],
   "source": [
    "print([list(map(int, x)) for x in v_data['maskrcnn_bboxes']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(v_data):\n",
    "    \"\"\"\n",
    "        Computes score for the two captions associated with the image\n",
    "\n",
    "        Args:\n",
    "            v_data (dict): A dictionary holding metadata about on one data sample\n",
    "\n",
    "        Returns:\n",
    "            score_c1 (float): Score for the first caption associated with the image\n",
    "            score_c2 (float): Score for the second caption associated with the image\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(BASE_DIR + 'models/' + model_name + '.pt')\n",
    "    combined_model.load_state_dict(checkpoint)\n",
    "    combined_model.to(device)\n",
    "    combined_model.eval()\n",
    "\n",
    "    img_path = os.path.join(DATA_DIR, v_data[\"img_local_path\"])\n",
    "    bbox_list = v_data['maskrcnn_bboxes']\n",
    "    bbox_classes = [-1] * len(bbox_list)\n",
    "    img = cv2.imread(img_path)\n",
    "    img_shape = img.shape[:2]\n",
    "    bbox_list.append([0, 0, img_shape[1], img_shape[0]])  # For entire image (global context)\n",
    "    bbox_classes.append(-1)\n",
    "    cap1 = v_data['caption1_modified']\n",
    "    cap2 = v_data['caption2_modified']\n",
    "\n",
    "    img_tensor = [torch.tensor(img).to(device)]\n",
    "    bboxes = [torch.tensor(bbox_list).to(device)]\n",
    "    bbox_classes = [torch.tensor(bbox_classes).to(device)]\n",
    "\n",
    "    if embed_type != 'use':\n",
    "        # For Glove, Fasttext embeddings\n",
    "        cap1_p = text_field.preprocess(cap1)\n",
    "        cap2_p = text_field.preprocess(cap2)\n",
    "        embed_c1 = torch.stack([text_field.vocab.vectors[text_field.vocab.stoi[x]] for x in cap1_p]).unsqueeze(\n",
    "            0).to(device)\n",
    "        embed_c2 = torch.stack([text_field.vocab.vectors[text_field.vocab.stoi[x]] for x in cap2_p]).unsqueeze(\n",
    "            0).to(device)\n",
    "    else:\n",
    "        # For USE embeddings\n",
    "        embed_c1 = torch.tensor(use_embed([cap1]).numpy()).to(device)\n",
    "        embed_c2 = torch.tensor(use_embed([cap2]).numpy()).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z_img, z_t_c1, z_t_c2 = combined_model(img_tensor, embed_c1, embed_c2, 1, [embed_c1.shape[1]],\n",
    "                                               [embed_c2.shape[1]], bboxes, bbox_classes)\n",
    "\n",
    "    z_img = z_img.permute(1, 0, 2)\n",
    "    z_text_c1 = z_t_c1.unsqueeze(2)\n",
    "    z_text_c2 = z_t_c2.unsqueeze(2)\n",
    "\n",
    "    # Compute Scores\n",
    "    score_c1 = torch.bmm(z_img, z_text_c1).squeeze()\n",
    "    score_c2 = torch.bmm(z_img, z_text_c2).squeeze()\n",
    "\n",
    "    return score_c1, score_c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ours_correct = 0\n",
    "lang_correct = 0\n",
    "\n",
    "for i, v_data in enumerate(test_samples):\n",
    "    actual_context = int(v_data['context_label'])\n",
    "    \n",
    "    language_context = 0 if float(v_data['bert_base_score']) >= textual_sim_threshold else 1\n",
    "\n",
    "    pred_context = evaluate_context_with_bbox_overlap(v_data)\n",
    "\n",
    "    if pred_context == actual_context:\n",
    "        ours_correct += 1\n",
    "\n",
    "    if language_context == actual_context:\n",
    "        lang_correct += 1\n",
    "\n",
    "print(\"Cosmos Accuracy\", ours_correct / len(test_samples))\n",
    "print(\"Language Baseline Accuracy\", lang_correct / len(test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}